{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-13T19:36:38.018966Z","iopub.execute_input":"2024-06-13T19:36:38.019754Z","iopub.status.idle":"2024-06-13T19:36:38.404116Z","shell.execute_reply.started":"2024-06-13T19:36:38.019722Z","shell.execute_reply":"2024-06-13T19:36:38.403359Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport random\n\nRANDOM_SEED = 42\ntorch.manual_seed(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\nnp.random.seed(RANDOM_SEED)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:36:39.481123Z","iopub.execute_input":"2024-06-13T19:36:39.481949Z","iopub.status.idle":"2024-06-13T19:36:42.664320Z","shell.execute_reply.started":"2024-06-13T19:36:39.481916Z","shell.execute_reply":"2024-06-13T19:36:42.663206Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"cornell-movie-review-data/rotten_tomatoes\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:36:42.665951Z","iopub.execute_input":"2024-06-13T19:36:42.666377Z","iopub.status.idle":"2024-06-13T19:36:53.804825Z","shell.execute_reply.started":"2024-06-13T19:36:42.666351Z","shell.execute_reply":"2024-06-13T19:36:53.803903Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/7.46k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f4008172e8f421ab0fb19b7934a5d00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/699k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0aa7a1ed6504456a43ddcb0ef0e7aad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/90.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d1786a7ddde48199ea33002d45cd4ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/92.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"178abb144fb8485ca2e65d6adfe6110e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/8530 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acd709e5a4984b6492e937458ebe4ba8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1066 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea4c763a3b844c0f931fee880caa9fdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1066 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd8d7926e79842f6863da82dd26bdf36"}},"metadata":{}}]},{"cell_type":"code","source":"unique, counts = np.unique(dataset['train']['label'], return_counts=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:36:53.805875Z","iopub.execute_input":"2024-06-13T19:36:53.806266Z","iopub.status.idle":"2024-06-13T19:36:53.820416Z","shell.execute_reply.started":"2024-06-13T19:36:53.806241Z","shell.execute_reply":"2024-06-13T19:36:53.819474Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"counts","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:36:53.823232Z","iopub.execute_input":"2024-06-13T19:36:53.823570Z","iopub.status.idle":"2024-06-13T19:36:53.833070Z","shell.execute_reply.started":"2024-06-13T19:36:53.823537Z","shell.execute_reply":"2024-06-13T19:36:53.832279Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"array([4265, 4265])"},"metadata":{}}]},{"cell_type":"markdown","source":"Нет дисбаланса классов, их строго одинаковое количество. \n\nПоскольку нет дисбаланса классов и при этом классы равнозначны, т.е. цена ошибки на обоих классах одинакова, то будем в дальнейшем в качестве метрики классификации использовать accuracy","metadata":{}},{"cell_type":"markdown","source":"Перейдем к модели","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer, BertModel\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\nmodel = BertModel.from_pretrained(\"bert-base-cased\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:36:53.834189Z","iopub.execute_input":"2024-06-13T19:36:53.834568Z","iopub.status.idle":"2024-06-13T19:37:20.636770Z","shell.execute_reply.started":"2024-06-13T19:36:53.834542Z","shell.execute_reply":"2024-06-13T19:37:20.635936Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c905fa6fb8a470fb7cbe7e9186fdfd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a146d952f0de4737922ae270c60966db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb24a4c341b04fc0882552d9f0a95f09"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dad3560ddad5461cad7972b99e4e8e6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59d9acf946ee468f94043b98c32d14c4"}},"metadata":{}}]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:37:20.638179Z","iopub.execute_input":"2024-06-13T19:37:20.638852Z","iopub.status.idle":"2024-06-13T19:37:20.645106Z","shell.execute_reply.started":"2024-06-13T19:37:20.638815Z","shell.execute_reply":"2024-06-13T19:37:20.644164Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 8530\n    })\n    validation: Dataset({\n        features: ['text', 'label'],\n        num_rows: 1066\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 1066\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"def tokenize_function(example):\n    return tokenizer(example[\"text\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:37:20.646254Z","iopub.execute_input":"2024-06-13T19:37:20.646616Z","iopub.status.idle":"2024-06-13T19:37:20.656159Z","shell.execute_reply.started":"2024-06-13T19:37:20.646592Z","shell.execute_reply":"2024-06-13T19:37:20.655294Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets = dataset.map(tokenize_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:37:20.657187Z","iopub.execute_input":"2024-06-13T19:37:20.657472Z","iopub.status.idle":"2024-06-13T19:37:29.935293Z","shell.execute_reply.started":"2024-06-13T19:37:20.657439Z","shell.execute_reply":"2024-06-13T19:37:29.934371Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8530 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5537f91357674142ac5774e72cfb2519"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1066 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd29744a7e174be8a507b72a8e203aa4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1066 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5904ebccf2ba4a68b51288f9a655a47f"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets['train'][1]","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:37:29.936657Z","iopub.execute_input":"2024-06-13T19:37:29.937034Z","iopub.status.idle":"2024-06-13T19:37:29.950422Z","shell.execute_reply.started":"2024-06-13T19:37:29.936999Z","shell.execute_reply":"2024-06-13T19:37:29.949471Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"{'text': 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .',\n 'label': 1,\n 'input_ids': [101,\n  1103,\n  10144,\n  1193,\n  9427,\n  14961,\n  1104,\n  107,\n  1103,\n  7692,\n  1104,\n  1103,\n  8374,\n  107,\n  14927,\n  1110,\n  1177,\n  3321,\n  1115,\n  170,\n  5551,\n  1104,\n  1734,\n  2834,\n  26449,\n  5594,\n  1884,\n  118,\n  2432,\n  120,\n  1900,\n  11109,\n  1200,\n  24498,\n  2142,\n  112,\n  188,\n  3631,\n  4152,\n  1104,\n  179,\n  119,\n  187,\n  119,\n  187,\n  119,\n  1106,\n  10493,\n  8584,\n  112,\n  188,\n  2243,\n  118,\n  4033,\n  119,\n  102],\n 'token_type_ids': [0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0],\n 'attention_mask': [1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1]}"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:37:29.955059Z","iopub.execute_input":"2024-06-13T19:37:29.955504Z","iopub.status.idle":"2024-06-13T19:37:41.458703Z","shell.execute_reply.started":"2024-06-13T19:37:29.955389Z","shell.execute_reply":"2024-06-13T19:37:41.457705Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"2024-06-13 19:37:31.932363: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-13 19:37:31.932477: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-13 19:37:32.062643: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"data_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:37:41.459967Z","iopub.execute_input":"2024-06-13T19:37:41.460670Z","iopub.status.idle":"2024-06-13T19:37:41.465026Z","shell.execute_reply.started":"2024-06-13T19:37:41.460640Z","shell.execute_reply":"2024-06-13T19:37:41.464114Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:37:41.466486Z","iopub.execute_input":"2024-06-13T19:37:41.466783Z","iopub.status.idle":"2024-06-13T19:37:41.494002Z","shell.execute_reply.started":"2024-06-13T19:37:41.466758Z","shell.execute_reply":"2024-06-13T19:37:41.492944Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 8530\n    })\n    validation: Dataset({\n        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 1066\n    })\n    test: Dataset({\n        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 1066\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\ntokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\ntokenized_datasets.set_format(\"torch\")\ntokenized_datasets[\"train\"].column_names","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:37:41.495295Z","iopub.execute_input":"2024-06-13T19:37:41.495648Z","iopub.status.idle":"2024-06-13T19:37:41.518218Z","shell.execute_reply.started":"2024-06-13T19:37:41.495621Z","shell.execute_reply":"2024-06-13T19:37:41.517354Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"['labels', 'input_ids', 'token_type_ids', 'attention_mask']"},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:37:41.519418Z","iopub.execute_input":"2024-06-13T19:37:41.519709Z","iopub.status.idle":"2024-06-13T19:37:41.525204Z","shell.execute_reply.started":"2024-06-13T19:37:41.519684Z","shell.execute_reply":"2024-06-13T19:37:41.524269Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 8530\n    })\n    validation: Dataset({\n        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 1066\n    })\n    test: Dataset({\n        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 1066\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(\n    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n)\neval_dataloader = DataLoader(\n    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:37:41.526350Z","iopub.execute_input":"2024-06-13T19:37:41.526623Z","iopub.status.idle":"2024-06-13T19:37:41.538282Z","shell.execute_reply.started":"2024-06-13T19:37:41.526600Z","shell.execute_reply":"2024-06-13T19:37:41.537356Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(\"Device used: {}.\".format(device))","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:37:41.539546Z","iopub.execute_input":"2024-06-13T19:37:41.539920Z","iopub.status.idle":"2024-06-13T19:37:41.571471Z","shell.execute_reply.started":"2024-06-13T19:37:41.539889Z","shell.execute_reply":"2024-06-13T19:37:41.570362Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Device used: cuda.\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch import nn\n\nin_features = 768\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n\nclass BertWithClassifier(nn.Module):\n    def __init__(self, linear_size):\n        super(BertWithClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(\"bert-base-cased\")\n        self.head = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(in_features=in_features, out_features=linear_size),\n            nn.BatchNorm1d(num_features=linear_size),\n            nn.Dropout(p=0.8),\n            nn.Linear(in_features=linear_size, out_features=1),\n            # nn.BatchNorm1d(num_features=1),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, tokens, attention_mask):\n        bert_output = self.bert(input_ids=tokens, attention_mask=attention_mask)\n        y = self.head(bert_output[1]) \n        return y\n        \n    def freeze_bert(self):\n        for param in self.bert.named_parameters():\n            param[1].requires_grad=False\n    \n    def unfreeze_bert(self):\n        for param in self.bert.named_parameters():\n            param[1].requires_grad=True\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:37:41.572789Z","iopub.execute_input":"2024-06-13T19:37:41.573081Z","iopub.status.idle":"2024-06-13T19:37:41.823125Z","shell.execute_reply.started":"2024-06-13T19:37:41.573057Z","shell.execute_reply":"2024-06-13T19:37:41.822356Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Пока потренируем только голову, это будет бейзлайном","metadata":{}},{"cell_type":"code","source":"# parameters\nnum_of_epochs = 24\nlearning_rate = 27e-6\nbatch_size = 16\nhidden_layers = 8\n\nprint(\"Epochs: {}\".format(num_of_epochs))\nprint(\"Learning rate: {:.6f}\".format(learning_rate))\nprint(\"Batch size: {}\".format(batch_size))\nprint(\"The number of hidden layers in the custom head: {}\".format(hidden_layers))","metadata":{"execution":{"iopub.status.busy":"2024-06-13T18:57:37.456715Z","iopub.execute_input":"2024-06-13T18:57:37.457064Z","iopub.status.idle":"2024-06-13T18:57:37.463417Z","shell.execute_reply.started":"2024-06-13T18:57:37.457030Z","shell.execute_reply":"2024-06-13T18:57:37.462426Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Epochs: 24\nLearning rate: 0.000027\nBatch size: 16\nThe number of hidden layers in the custom head: 8\n","output_type":"stream"}]},{"cell_type":"code","source":"model = BertWithClassifier(linear_size=hidden_layers)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T18:57:37.464765Z","iopub.execute_input":"2024-06-13T18:57:37.465100Z","iopub.status.idle":"2024-06-13T18:57:38.216879Z","shell.execute_reply.started":"2024-06-13T18:57:37.465076Z","shell.execute_reply":"2024-06-13T18:57:38.215899Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"BertWithClassifier(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (head): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=768, out_features=8, bias=True)\n    (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Dropout(p=0.8, inplace=False)\n    (4): Linear(in_features=8, out_features=1, bias=True)\n    (5): Sigmoid()\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AdamW\n\n\noptimizer = AdamW(model.parameters(), lr=learning_rate)\nloss_fn = nn.BCELoss()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:37:41.824247Z","iopub.execute_input":"2024-06-13T19:37:41.824556Z","iopub.status.idle":"2024-06-13T19:37:42.501765Z","shell.execute_reply.started":"2024-06-13T19:37:41.824531Z","shell.execute_reply":"2024-06-13T19:37:42.499945Z"},"trusted":true},"execution_count":19,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdamW\n\u001b[0;32m----> 4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[43mlearning_rate\u001b[49m)\n\u001b[1;32m      5\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n","\u001b[0;31mNameError\u001b[0m: name 'learning_rate' is not defined"],"ename":"NameError","evalue":"name 'learning_rate' is not defined","output_type":"error"}]},{"cell_type":"code","source":"from transformers import get_scheduler\n\nnum_training_steps = num_of_epochs * len(train_dataloader)\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps,\n)\nprint(num_training_steps)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:37:42.502552Z","iopub.status.idle":"2024-06-13T19:37:42.502913Z","shell.execute_reply.started":"2024-06-13T19:37:42.502729Z","shell.execute_reply":"2024-06-13T19:37:42.502761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.train()\nmodel.freeze_bert()","metadata":{"execution":{"iopub.status.busy":"2024-06-12T07:51:02.588511Z","iopub.execute_input":"2024-06-12T07:51:02.588847Z","iopub.status.idle":"2024-06-12T07:51:02.597191Z","shell.execute_reply.started":"2024-06-12T07:51:02.588810Z","shell.execute_reply":"2024-06-12T07:51:02.596419Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"for batch in train_dataloader:\n    break\n{k: v.shape for k, v in batch.items()}","metadata":{"execution":{"iopub.status.busy":"2024-06-12T07:51:02.598305Z","iopub.execute_input":"2024-06-12T07:51:02.598565Z","iopub.status.idle":"2024-06-12T07:51:02.623310Z","shell.execute_reply.started":"2024-06-12T07:51:02.598541Z","shell.execute_reply":"2024-06-12T07:51:02.622454Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"{'labels': torch.Size([8]),\n 'input_ids': torch.Size([8, 49]),\n 'token_type_ids': torch.Size([8, 49]),\n 'attention_mask': torch.Size([8, 49])}"},"metadata":{}}]},{"cell_type":"code","source":"from tqdm.auto import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:39:57.432821Z","iopub.execute_input":"2024-06-13T19:39:57.433615Z","iopub.status.idle":"2024-06-13T19:39:57.437776Z","shell.execute_reply.started":"2024-06-13T19:39:57.433584Z","shell.execute_reply":"2024-06-13T19:39:57.436762Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"\n\nprogress_bar = tqdm(range(num_training_steps))\n\nmodel.train()\nmodel.freeze_bert()\n\nfor epoch in range(num_of_epochs):\n    for batch in train_dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = torch.flatten(model(tokens=batch['input_ids'], attention_mask=batch['attention_mask']))\n        loss = loss_fn(outputs, batch['labels'].float())\n        loss.backward()\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T07:51:02.624342Z","iopub.execute_input":"2024-06-12T07:51:02.624594Z","iopub.status.idle":"2024-06-12T07:57:46.526811Z","shell.execute_reply.started":"2024-06-12T07:51:02.624571Z","shell.execute_reply":"2024-06-12T07:57:46.526047Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25608 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac75b571f2d049e884f83d102e1e4d7d"}},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import f1_score, accuracy_score\n\ndef eval_prediction(y_batch_actual, y_batch_predicted):\n    \"\"\"Return batches of accuracy and f1 scores.\"\"\"\n    y_batch_actual_np = y_batch_actual.cpu().detach().numpy()\n    y_batch_predicted_np = np.round(y_batch_predicted.cpu().detach().numpy())\n    \n    acc = accuracy_score(y_true=y_batch_actual_np, y_pred=y_batch_predicted_np)\n    f1 = f1_score(y_true=y_batch_actual_np, y_pred=y_batch_predicted_np, average='weighted')\n    \n    return acc, f1","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:42:07.614330Z","iopub.execute_input":"2024-06-13T19:42:07.614704Z","iopub.status.idle":"2024-06-13T19:42:07.622067Z","shell.execute_reply.started":"2024-06-13T19:42:07.614676Z","shell.execute_reply":"2024-06-13T19:42:07.620958Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"!pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:42:09.872982Z","iopub.execute_input":"2024-06-13T19:42:09.873353Z","iopub.status.idle":"2024-06-13T19:42:24.034186Z","shell.execute_reply.started":"2024-06-13T19:42:09.873325Z","shell.execute_reply":"2024-06-13T19:42:24.032939Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.19.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.1)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.3.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.23.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.2\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\nmodel.freeze_bert()\n\nsize = len(eval_dataloader)\nf1, acc = 0, 0\n\nwith torch.no_grad():\n    for batch in eval_dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        X = batch['input_ids']\n        attention_mask = batch['attention_mask']\n        y = batch['labels']\n\n        pred = model(tokens=X, attention_mask=attention_mask)\n\n        acc_batch, f1_batch = eval_prediction(y.float(), pred)                        \n        acc += acc_batch\n        f1 += f1_batch\n\n    acc = acc/size\n    f1 = f1/size\n\nacc, f1","metadata":{"execution":{"iopub.status.busy":"2024-06-12T07:58:00.108909Z","iopub.execute_input":"2024-06-12T07:58:00.109262Z","iopub.status.idle":"2024-06-12T07:58:02.763596Z","shell.execute_reply.started":"2024-06-12T07:58:00.109231Z","shell.execute_reply":"2024-06-12T07:58:02.762684Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"(0.6222014925373134, 0.7495407991676658)"},"metadata":{}}]},{"cell_type":"code","source":"# model = BertWithClassifier(linear_size=hidden_layers)\n# model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T07:58:02.764868Z","iopub.execute_input":"2024-06-12T07:58:02.765221Z","iopub.status.idle":"2024-06-12T07:58:02.769523Z","shell.execute_reply.started":"2024-06-12T07:58:02.765187Z","shell.execute_reply":"2024-06-12T07:58:02.768448Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\nprogress_bar = tqdm(range(num_training_steps))\n\nmodel.train()\nmodel.unfreeze_bert()\n\nfor epoch in range(num_of_epochs):\n    for batch in train_dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = torch.flatten(model(tokens=batch['input_ids'], attention_mask=batch['attention_mask']))\n        loss = loss_fn(outputs, batch['labels'].float())\n        loss.backward()\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T07:58:02.770816Z","iopub.execute_input":"2024-06-12T07:58:02.771160Z","iopub.status.idle":"2024-06-12T08:22:51.995225Z","shell.execute_reply.started":"2024-06-12T07:58:02.771125Z","shell.execute_reply":"2024-06-12T08:22:51.994210Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25608 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e0c029129734abb88ae6e3ffa1bf9af"}},"metadata":{}}]},{"cell_type":"code","source":"model.eval()\nmodel.freeze_bert()\n\nsize = len(eval_dataloader)\nf1, acc = 0, 0\n\nwith torch.no_grad():\n    for batch in eval_dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        X = batch['input_ids']\n        attention_mask = batch['attention_mask']\n        y = batch['labels']\n\n        pred = model(tokens=X, attention_mask=attention_mask)\n\n        acc_batch, f1_batch = eval_prediction(y.float(), pred)                        \n        acc += acc_batch\n        f1 += f1_batch\n\n    acc = acc/size\n    f1 = f1/size\n\nacc, f1","metadata":{"execution":{"iopub.status.busy":"2024-06-12T08:22:51.996566Z","iopub.execute_input":"2024-06-12T08:22:51.996877Z","iopub.status.idle":"2024-06-12T08:22:54.622672Z","shell.execute_reply.started":"2024-06-12T08:22:51.996852Z","shell.execute_reply":"2024-06-12T08:22:54.621702Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"(0.6231343283582089, 0.7514877742489693)"},"metadata":{}}]},{"cell_type":"code","source":"def training_step(dataloader, model, optimizer, loss_fn, if_freeze_bert):\n    \"\"\"Method to train the model\"\"\"\n    \n    model.train()\n    model.freeze_bert() if if_freeze_bert else model.unfreeze_bert()\n      \n    epoch_loss = 0\n    size = len(dataloader.dataset)\n \n    for i, batch in enumerate(dataloader):        \n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n    \n        outputs = torch.flatten(model(tokens=input_ids, attention_mask=attention_mask))\n                        \n        optimizer.zero_grad()\n        loss = loss_fn(outputs, labels.float())\n        epoch_loss += loss.item()\n        loss.backward()\n        optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:37:42.508852Z","iopub.status.idle":"2024-06-13T19:37:42.509173Z","shell.execute_reply.started":"2024-06-13T19:37:42.509016Z","shell.execute_reply":"2024-06-13T19:37:42.509029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validation_step(dataloader, model, loss_fn):\n    \"\"\"Method to test the model's accuracy and loss on the validation set\"\"\"\n    \n    model.eval()\n    model.freeze_bert()\n    \n    size = len(dataloader)\n    f1, acc = 0, 0\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            X = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            y = batch['labels'].to(device)\n                  \n            pred = model(tokens=X, attention_mask=attention_mask)\n            \n            acc_batch, f1_batch = eval_prediction(y.float(), pred)                        \n            acc += acc_batch\n            f1 += f1_batch\n\n        acc = acc/size\n        f1 = f1/size\n                \n    return acc, f1","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:37:42.511115Z","iopub.status.idle":"2024-06-13T19:37:42.511482Z","shell.execute_reply.started":"2024-06-13T19:37:42.511278Z","shell.execute_reply":"2024-06-13T19:37:42.511291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BertWithClassifier(linear_size=hidden_layers)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T08:22:54.646410Z","iopub.execute_input":"2024-06-12T08:22:54.646692Z","iopub.status.idle":"2024-06-12T08:22:55.239423Z","shell.execute_reply.started":"2024-06-12T08:22:54.646669Z","shell.execute_reply":"2024-06-12T08:22:55.238557Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"BertWithClassifier(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (head): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=768, out_features=8, bias=True)\n    (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Dropout(p=0.8, inplace=False)\n    (4): Linear(in_features=8, out_features=1, bias=True)\n    (5): Sigmoid()\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# parameters\n# num_of_epochs = 24\nnum_of_epochs = 13\n# learning_rate = 1e-6\nlearning_rate = 1e-5\nbatch_size = 16\nhidden_layers = 8\n\nprint(\"Epochs: {}\".format(num_of_epochs))\nprint(\"Learning rate: {:.6f}\".format(learning_rate))\nprint(\"Batch size: {}\".format(batch_size))\nprint(\"The number of hidden layers in the custom head: {}\".format(hidden_layers))","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:39:24.830782Z","iopub.execute_input":"2024-06-13T19:39:24.831138Z","iopub.status.idle":"2024-06-13T19:39:24.838360Z","shell.execute_reply.started":"2024-06-13T19:39:24.831111Z","shell.execute_reply":"2024-06-13T19:39:24.837353Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Epochs: 13\nLearning rate: 0.000010\nBatch size: 16\nThe number of hidden layers in the custom head: 8\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=learning_rate)\nloss_fn = nn.BCELoss()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:37:42.515098Z","iopub.status.idle":"2024-06-13T19:37:42.515441Z","shell.execute_reply.started":"2024-06-13T19:37:42.515253Z","shell.execute_reply":"2024-06-13T19:37:42.515266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\ntqdm.pandas()\n\nbest_acc, best_f1 = 0, 0\npath = './best_model.pt'\nif_freeze_bert = False\n\nfor i in tqdm(range(num_of_epochs)):\n    print(\"Epoch: #{}\".format(i+1))\n\n    if i < 5:\n        if_freeze_bert = True\n        print(\"Bert is freezed\")\n    else:\n        if_freeze_bert = False\n        print(\"Bert is not freezed\")\n    \n    training_step(train_dataloader, model,optimizer, loss_fn, if_freeze_bert)\n    train_acc, train_f1 = validation_step(train_dataloader, model, loss_fn)\n    val_acc, val_f1 = validation_step(eval_dataloader, model, loss_fn)\n    \n    print(\"Training results: \")\n    print(\"Acc: {:.3f}, f1: {:.3f}\".format(train_acc, train_f1))\n    \n    print(\"Validation results: \")\n    print(\"Acc: {:.3f}, f1: {:.3f}\".format(val_acc, val_f1))\n    \n    if val_acc > best_acc:\n        best_acc = val_acc    \n        torch.save(model, path)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T08:22:55.276293Z","iopub.execute_input":"2024-06-12T08:22:55.276534Z","iopub.status.idle":"2024-06-12T08:38:09.521414Z","shell.execute_reply.started":"2024-06-12T08:22:55.276513Z","shell.execute_reply":"2024-06-12T08:38:09.520544Z"},"trusted":true},"execution_count":38,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ea26bbc3ffa40a6ad0ce2fa98d4f120"}},"metadata":{}},{"name":"stdout","text":"Epoch: #1\nBert is freezed\nTraining results: \nAcc: 0.502, f1: 0.358\nValidation results: \nAcc: 0.507, f1: 0.510\nEpoch: #2\nBert is freezed\nTraining results: \nAcc: 0.513, f1: 0.381\nValidation results: \nAcc: 0.511, f1: 0.526\nEpoch: #3\nBert is freezed\nTraining results: \nAcc: 0.544, f1: 0.455\nValidation results: \nAcc: 0.556, f1: 0.614\nEpoch: #4\nBert is freezed\nTraining results: \nAcc: 0.554, f1: 0.479\nValidation results: \nAcc: 0.564, f1: 0.634\nEpoch: #5\nBert is freezed\nTraining results: \nAcc: 0.553, f1: 0.476\nValidation results: \nAcc: 0.560, f1: 0.626\nEpoch: #6\nBert is not freezed\nTraining results: \nAcc: 0.886, f1: 0.885\nValidation results: \nAcc: 0.839, f1: 0.906\nEpoch: #7\nBert is not freezed\nTraining results: \nAcc: 0.906, f1: 0.905\nValidation results: \nAcc: 0.834, f1: 0.902\nEpoch: #8\nBert is not freezed\nTraining results: \nAcc: 0.944, f1: 0.944\nValidation results: \nAcc: 0.866, f1: 0.923\nEpoch: #9\nBert is not freezed\nTraining results: \nAcc: 0.947, f1: 0.948\nValidation results: \nAcc: 0.844, f1: 0.907\nEpoch: #10\nBert is not freezed\nTraining results: \nAcc: 0.969, f1: 0.969\nValidation results: \nAcc: 0.826, f1: 0.897\nEpoch: #11\nBert is not freezed\nTraining results: \nAcc: 0.983, f1: 0.982\nValidation results: \nAcc: 0.856, f1: 0.918\nEpoch: #12\nBert is not freezed\nTraining results: \nAcc: 0.985, f1: 0.985\nValidation results: \nAcc: 0.868, f1: 0.925\nEpoch: #13\nBert is not freezed\nTraining results: \nAcc: 0.988, f1: 0.988\nValidation results: \nAcc: 0.847, f1: 0.912\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Для базового Embedding MixUp буду опираться на эту статью: https://aclanthology.org/2020.coling-main.305.pdf","metadata":{}},{"cell_type":"code","source":"best_acc","metadata":{"execution":{"iopub.status.busy":"2024-06-12T08:38:09.522677Z","iopub.execute_input":"2024-06-12T08:38:09.522982Z","iopub.status.idle":"2024-06-12T08:38:09.528679Z","shell.execute_reply.started":"2024-06-12T08:38:09.522957Z","shell.execute_reply":"2024-06-12T08:38:09.527806Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"0.8684701492537313"},"metadata":{}}]},{"cell_type":"code","source":"# next(iter(train_dataloader))['input_ids'].shape","metadata":{"execution":{"iopub.status.busy":"2024-06-12T08:38:09.529849Z","iopub.execute_input":"2024-06-12T08:38:09.530089Z","iopub.status.idle":"2024-06-12T08:38:09.537711Z","shell.execute_reply.started":"2024-06-12T08:38:09.530067Z","shell.execute_reply":"2024-06-12T08:38:09.536802Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"from torch import nn\n\nin_features = 768\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n\nclass BertWithClassifierMixUp(nn.Module):\n    def __init__(self, linear_size):\n        super(BertWithClassifierMixUp, self).__init__()\n        self.bert = BertModel.from_pretrained(\"bert-base-cased\")\n        self.head = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(in_features=in_features, out_features=linear_size),\n            nn.BatchNorm1d(num_features=linear_size),\n            nn.Dropout(p=0.8),\n            nn.Linear(in_features=linear_size, out_features=1),\n            # nn.BatchNorm1d(num_features=1),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, tokens, attention_mask):\n        bert_output = self.bert(input_ids=tokens, attention_mask=attention_mask)\n        y = self.head(bert_output[1]) \n        return y\n    \n    def forward_mixup(self, tokens1, attention_mask1, tokens2, attention_mask2, lam):\n        bert_output1 = self.bert(input_ids=tokens1, attention_mask=attention_mask1)\n        bert_output2 = self.bert(input_ids=tokens2, attention_mask=attention_mask2)\n    \n        bert_output = lam * bert_output1[1] + (1.0 - lam) * bert_output2[1]\n    \n        y = self.head(bert_output) \n        return y\n        \n    def freeze_bert(self):\n        for param in self.bert.named_parameters():\n            param[1].requires_grad=False\n    \n    def unfreeze_bert(self):\n        for param in self.bert.named_parameters():\n            param[1].requires_grad=True\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:37:42.516348Z","iopub.status.idle":"2024-06-13T19:37:42.516673Z","shell.execute_reply.started":"2024-06-13T19:37:42.516507Z","shell.execute_reply":"2024-06-13T19:37:42.516520Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def training_step_with_mixup(dataloader, model, optimizer, loss_fn, if_freeze_bert, lam):\n    \"\"\"Method to train the model\"\"\"\n    \n    model.train()\n    model.freeze_bert() if if_freeze_bert else model.unfreeze_bert()\n      \n    epoch_loss = 0\n    size = len(dataloader.dataset)\n \n    for i, batch in enumerate(dataloader):\n        \n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n#         batch2 = next(iter(dataloader))\n#         input_ids2 = batch2['input_ids'].to(device)\n#         attention_mask2 = batch2['attention_mask'].to(device)\n#         labels2 = batch2['labels'].to(device)\n        \n#         print(input_ids.shape)\n#         print(attention_mask.shape)\n#         print(labels.shape)\n        \n    \n        model_answer = model.forward_mixup(input_ids, attention_mask, torch.flip(input_ids, dims=(0,)), torch.flip(attention_mask, dims=(0,)), lam)\n    \n        outputs = torch.flatten(model_answer)\n        \n        mixup_labels = lam * labels.float() + (1.0 - lam) * torch.flip(labels.float(), dims=(0,))\n        \n        optimizer.zero_grad()\n        loss = loss_fn(outputs, mixup_labels)\n        epoch_loss += loss.item()\n        loss.backward()\n        optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:37:42.518491Z","iopub.status.idle":"2024-06-13T19:37:42.518828Z","shell.execute_reply.started":"2024-06-13T19:37:42.518665Z","shell.execute_reply":"2024-06-13T19:37:42.518679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nmodel_mixup = BertWithClassifierMixUp(linear_size=hidden_layers)\nmodel_mixup.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T08:38:09.753095Z","iopub.execute_input":"2024-06-12T08:38:09.753377Z","iopub.status.idle":"2024-06-12T08:38:10.299263Z","shell.execute_reply.started":"2024-06-12T08:38:09.753351Z","shell.execute_reply":"2024-06-12T08:38:10.298349Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"BertWithClassifierMixUp(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (head): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=768, out_features=8, bias=True)\n    (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): Dropout(p=0.8, inplace=False)\n    (4): Linear(in_features=8, out_features=1, bias=True)\n    (5): Sigmoid()\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"optimizer = AdamW(model_mixup.parameters(), lr=learning_rate)\nloss_fn = nn.BCELoss()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:37:51.013718Z","iopub.execute_input":"2024-06-13T19:37:51.014083Z","iopub.status.idle":"2024-06-13T19:37:51.059896Z","shell.execute_reply.started":"2024-06-13T19:37:51.014053Z","shell.execute_reply":"2024-06-13T19:37:51.058706Z"},"trusted":true},"execution_count":20,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m AdamW(\u001b[43mmodel_mixup\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m      2\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n","\u001b[0;31mNameError\u001b[0m: name 'model_mixup' is not defined"],"ename":"NameError","evalue":"name 'model_mixup' is not defined","output_type":"error"}]},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\ntqdm.pandas()\n\nbest_acc, best_f1 = 0, 0\npath = './best_model.pt'\nif_freeze_bert = False\nlam = 1\n\nfor i in tqdm(range(num_of_epochs)):\n    print(\"Epoch: #{}\".format(i+1))\n\n    if i < 5:\n        if_freeze_bert = True\n        print(\"Bert is freezed\")\n        lam = 1\n    else:\n        if_freeze_bert = False\n        lam = 0.7\n        print(\"Bert is not freezed\")\n    \n    training_step_with_mixup(train_dataloader, model_mixup, optimizer, loss_fn, if_freeze_bert, lam)\n    train_acc, train_f1 = validation_step(train_dataloader, model_mixup, loss_fn)\n    val_acc, val_f1 = validation_step(eval_dataloader, model_mixup, loss_fn)\n    \n    print(\"Training results: \")\n    print(\"Acc: {:.3f}, f1: {:.3f}\".format(train_acc, train_f1))\n    \n    print(\"Validation results: \")\n    print(\"Acc: {:.3f}, f1: {:.3f}\".format(val_acc, val_f1))\n    \n    if val_acc > best_acc:\n        best_acc = val_acc    \n        torch.save(model, path)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T08:38:10.311393Z","iopub.execute_input":"2024-06-12T08:38:10.311687Z","iopub.status.idle":"2024-06-12T09:01:03.311253Z","shell.execute_reply.started":"2024-06-12T08:38:10.311657Z","shell.execute_reply":"2024-06-12T09:01:03.310268Z"},"trusted":true},"execution_count":45,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cba76c986945478e9182293a826c54d3"}},"metadata":{}},{"name":"stdout","text":"Epoch: #1\nBert is freezed\nTraining results: \nAcc: 0.565, f1: 0.496\nValidation results: \nAcc: 0.583, f1: 0.655\nEpoch: #2\nBert is freezed\nTraining results: \nAcc: 0.609, f1: 0.609\nValidation results: \nAcc: 0.597, f1: 0.725\nEpoch: #3\nBert is freezed\nTraining results: \nAcc: 0.591, f1: 0.578\nValidation results: \nAcc: 0.580, f1: 0.694\nEpoch: #4\nBert is freezed\nTraining results: \nAcc: 0.609, f1: 0.608\nValidation results: \nAcc: 0.608, f1: 0.738\nEpoch: #5\nBert is freezed\nTraining results: \nAcc: 0.618, f1: 0.609\nValidation results: \nAcc: 0.615, f1: 0.739\nEpoch: #6\nBert is not freezed\nTraining results: \nAcc: 0.860, f1: 0.860\nValidation results: \nAcc: 0.825, f1: 0.896\nEpoch: #7\nBert is not freezed\nTraining results: \nAcc: 0.895, f1: 0.894\nValidation results: \nAcc: 0.840, f1: 0.905\nEpoch: #8\nBert is not freezed\nTraining results: \nAcc: 0.918, f1: 0.918\nValidation results: \nAcc: 0.851, f1: 0.913\nEpoch: #9\nBert is not freezed\nTraining results: \nAcc: 0.930, f1: 0.930\nValidation results: \nAcc: 0.835, f1: 0.902\nEpoch: #10\nBert is not freezed\nTraining results: \nAcc: 0.935, f1: 0.934\nValidation results: \nAcc: 0.837, f1: 0.901\nEpoch: #11\nBert is not freezed\nTraining results: \nAcc: 0.963, f1: 0.963\nValidation results: \nAcc: 0.841, f1: 0.908\nEpoch: #12\nBert is not freezed\nTraining results: \nAcc: 0.972, f1: 0.972\nValidation results: \nAcc: 0.857, f1: 0.918\nEpoch: #13\nBert is not freezed\nTraining results: \nAcc: 0.975, f1: 0.975\nValidation results: \nAcc: 0.853, f1: 0.914\n","output_type":"stream"}]},{"cell_type":"code","source":"best_acc","metadata":{"execution":{"iopub.status.busy":"2024-06-12T09:01:03.312367Z","iopub.execute_input":"2024-06-12T09:01:03.312637Z","iopub.status.idle":"2024-06-12T09:01:03.318597Z","shell.execute_reply.started":"2024-06-12T09:01:03.312612Z","shell.execute_reply":"2024-06-12T09:01:03.317730Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"0.8572761194029851"},"metadata":{}}]},{"cell_type":"code","source":"model_mixup = BertWithClassifierMixUp(linear_size=hidden_layers)\nmodel_mixup.to(device)\noptimizer = AdamW(model_mixup.parameters(), lr=learning_rate)\nloss_fn = nn.BCELoss()\n\ntqdm.pandas()\n\nbest_acc, best_f1 = 0, 0\npath = './best_model.pt'\nif_freeze_bert = False\nlam = 1\n\nfor i in tqdm(range(num_of_epochs)):\n    print(\"Epoch: #{}\".format(i+1))\n\n    if i < 5:\n        if_freeze_bert = True\n        print(\"Bert is freezed\")\n        lam = 1\n    else:\n        if_freeze_bert = False\n        lam = 0.85\n        print(\"Bert is not freezed\")\n    \n    training_step_with_mixup(train_dataloader, model_mixup, optimizer, loss_fn, if_freeze_bert, lam)\n    train_acc, train_f1 = validation_step(train_dataloader, model_mixup, loss_fn)\n    val_acc, val_f1 = validation_step(eval_dataloader, model_mixup, loss_fn)\n    \n    print(\"Training results: \")\n    print(\"Acc: {:.3f}, f1: {:.3f}\".format(train_acc, train_f1))\n    \n    print(\"Validation results: \")\n    print(\"Acc: {:.3f}, f1: {:.3f}\".format(val_acc, val_f1))\n    \n    if val_acc > best_acc:\n        best_acc = val_acc    \n        torch.save(model, path)\n        \nbest_acc","metadata":{"execution":{"iopub.status.busy":"2024-06-12T09:57:44.258748Z","iopub.execute_input":"2024-06-12T09:57:44.259399Z","iopub.status.idle":"2024-06-12T10:20:36.621989Z","shell.execute_reply.started":"2024-06-12T09:57:44.259368Z","shell.execute_reply":"2024-06-12T10:20:36.620994Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"486f64796dba4036a1ffa69cb3f21559"}},"metadata":{}},{"name":"stdout","text":"Epoch: #1\nBert is freezed\nTraining results: \nAcc: 0.492, f1: 0.359\nValidation results: \nAcc: 0.503, f1: 0.516\nEpoch: #2\nBert is freezed\nTraining results: \nAcc: 0.501, f1: 0.356\nValidation results: \nAcc: 0.497, f1: 0.496\nEpoch: #3\nBert is freezed\nTraining results: \nAcc: 0.534, f1: 0.440\nValidation results: \nAcc: 0.525, f1: 0.564\nEpoch: #4\nBert is freezed\nTraining results: \nAcc: 0.557, f1: 0.494\nValidation results: \nAcc: 0.552, f1: 0.628\nEpoch: #5\nBert is freezed\nTraining results: \nAcc: 0.574, f1: 0.534\nValidation results: \nAcc: 0.563, f1: 0.653\nEpoch: #6\nBert is not freezed\nTraining results: \nAcc: 0.873, f1: 0.873\nValidation results: \nAcc: 0.835, f1: 0.903\nEpoch: #7\nBert is not freezed\nTraining results: \nAcc: 0.908, f1: 0.908\nValidation results: \nAcc: 0.838, f1: 0.907\nEpoch: #8\nBert is not freezed\nTraining results: \nAcc: 0.928, f1: 0.928\nValidation results: \nAcc: 0.850, f1: 0.913\nEpoch: #9\nBert is not freezed\nTraining results: \nAcc: 0.945, f1: 0.945\nValidation results: \nAcc: 0.855, f1: 0.917\nEpoch: #10\nBert is not freezed\nTraining results: \nAcc: 0.963, f1: 0.963\nValidation results: \nAcc: 0.847, f1: 0.912\nEpoch: #11\nBert is not freezed\nTraining results: \nAcc: 0.972, f1: 0.972\nValidation results: \nAcc: 0.852, f1: 0.915\nEpoch: #12\nBert is not freezed\nTraining results: \nAcc: 0.974, f1: 0.975\nValidation results: \nAcc: 0.854, f1: 0.916\nEpoch: #13\nBert is not freezed\nTraining results: \nAcc: 0.979, f1: 0.979\nValidation results: \nAcc: 0.851, f1: 0.914\n","output_type":"stream"},{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"0.855410447761194"},"metadata":{}}]},{"cell_type":"code","source":"model_mixup = BertWithClassifierMixUp(linear_size=hidden_layers)\nmodel_mixup.to(device)\noptimizer = AdamW(model_mixup.parameters(), lr=learning_rate)\nloss_fn = nn.BCELoss()\n\ntqdm.pandas()\n\nbest_acc, best_f1 = 0, 0\npath = './best_model.pt'\nif_freeze_bert = False\nlam = 1\n\nfor i in tqdm(range(num_of_epochs)):\n    print(\"Epoch: #{}\".format(i+1))\n\n    if i < 3:\n        if_freeze_bert = True\n        print(\"Bert is freezed\")\n        lam = 1\n    else:\n        if_freeze_bert = False\n        lam = 0.6\n        print(\"Bert is not freezed\")\n    \n    training_step_with_mixup(train_dataloader, model_mixup, optimizer, loss_fn, if_freeze_bert, lam)\n    train_acc, train_f1 = validation_step(train_dataloader, model_mixup, loss_fn)\n    val_acc, val_f1 = validation_step(eval_dataloader, model_mixup, loss_fn)\n    \n    print(\"Training results: \")\n    print(\"Acc: {:.3f}, f1: {:.3f}\".format(train_acc, train_f1))\n    \n    print(\"Validation results: \")\n    print(\"Acc: {:.3f}, f1: {:.3f}\".format(val_acc, val_f1))\n    \n    if val_acc > best_acc:\n        best_acc = val_acc    \n        torch.save(model, path)\n        \nbest_acc","metadata":{"execution":{"iopub.status.busy":"2024-06-12T09:26:23.402636Z","iopub.execute_input":"2024-06-12T09:26:23.402944Z","iopub.status.idle":"2024-06-12T09:51:46.579331Z","shell.execute_reply.started":"2024-06-12T09:26:23.402919Z","shell.execute_reply":"2024-06-12T09:51:46.578432Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b0cc2ef40964f239a9eb09ec7dd5e82"}},"metadata":{}},{"name":"stdout","text":"Epoch: #1\nBert is freezed\nTraining results: \nAcc: 0.500, f1: 0.353\nValidation results: \nAcc: 0.503, f1: 0.502\nEpoch: #2\nBert is freezed\nTraining results: \nAcc: 0.506, f1: 0.366\nValidation results: \nAcc: 0.505, f1: 0.511\nEpoch: #3\nBert is freezed\nTraining results: \nAcc: 0.531, f1: 0.421\nValidation results: \nAcc: 0.535, f1: 0.574\nEpoch: #4\nBert is not freezed\nTraining results: \nAcc: 0.822, f1: 0.822\nValidation results: \nAcc: 0.815, f1: 0.890\nEpoch: #5\nBert is not freezed\nTraining results: \nAcc: 0.866, f1: 0.866\nValidation results: \nAcc: 0.827, f1: 0.897\nEpoch: #6\nBert is not freezed\nTraining results: \nAcc: 0.886, f1: 0.886\nValidation results: \nAcc: 0.835, f1: 0.902\nEpoch: #7\nBert is not freezed\nTraining results: \nAcc: 0.902, f1: 0.902\nValidation results: \nAcc: 0.844, f1: 0.908\nEpoch: #8\nBert is not freezed\nTraining results: \nAcc: 0.924, f1: 0.924\nValidation results: \nAcc: 0.844, f1: 0.910\nEpoch: #9\nBert is not freezed\nTraining results: \nAcc: 0.938, f1: 0.938\nValidation results: \nAcc: 0.846, f1: 0.910\nEpoch: #10\nBert is not freezed\nTraining results: \nAcc: 0.944, f1: 0.944\nValidation results: \nAcc: 0.853, f1: 0.915\nEpoch: #11\nBert is not freezed\nTraining results: \nAcc: 0.955, f1: 0.955\nValidation results: \nAcc: 0.846, f1: 0.910\nEpoch: #12\nBert is not freezed\nTraining results: \nAcc: 0.963, f1: 0.963\nValidation results: \nAcc: 0.838, f1: 0.905\nEpoch: #13\nBert is not freezed\nTraining results: \nAcc: 0.966, f1: 0.966\nValidation results: \nAcc: 0.839, f1: 0.906\n","output_type":"stream"},{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"0.8526119402985075"},"metadata":{}}]},{"cell_type":"code","source":"model_mixup = BertWithClassifierMixUp(linear_size=hidden_layers)\nmodel_mixup.to(device)\noptimizer = AdamW(model_mixup.parameters(), lr=learning_rate)\nloss_fn = nn.BCELoss()\n\ntqdm.pandas()\n\nbest_acc, best_f1 = 0, 0\npath = './best_model_with_mix.pt'\nif_freeze_bert = False\nlam = 1\n\nnum_of_epochs = 20\n\nfor i in tqdm(range(num_of_epochs)):\n    print(\"Epoch: #{}\".format(i+1))\n\n    if i < 5:\n        if_freeze_bert = True\n        print(\"Bert is freezed\")\n        lam = 1\n    else:\n        if_freeze_bert = False\n        print(\"Bert is not freezed\")\n    \n    if i >= 10:\n        lam = 0.9\n        print(\"mix_up_on\")\n        \n    \n    training_step_with_mixup(train_dataloader, model_mixup, optimizer, loss_fn, if_freeze_bert, lam)\n    train_acc, train_f1 = validation_step(train_dataloader, model_mixup, loss_fn)\n    val_acc, val_f1 = validation_step(eval_dataloader, model_mixup, loss_fn)\n    \n    print(\"Training results: \")\n    print(\"Acc: {:.3f}, f1: {:.3f}\".format(train_acc, train_f1))\n    \n    print(\"Validation results: \")\n    print(\"Acc: {:.3f}, f1: {:.3f}\".format(val_acc, val_f1))\n    \n    if val_acc > best_acc:\n        best_acc = val_acc    \n        torch.save(model_mixup, path)\n        \nbest_acc","metadata":{"execution":{"iopub.status.busy":"2024-06-12T10:20:36.623512Z","iopub.execute_input":"2024-06-12T10:20:36.623828Z","iopub.status.idle":"2024-06-12T10:59:12.054313Z","shell.execute_reply.started":"2024-06-12T10:20:36.623802Z","shell.execute_reply":"2024-06-12T10:59:12.053285Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05ac583392ba42e68219a8d07b343d75"}},"metadata":{}},{"name":"stdout","text":"Epoch: #1\nBert is freezed\nTraining results: \nAcc: 0.568, f1: 0.545\nValidation results: \nAcc: 0.584, f1: 0.705\nEpoch: #2\nBert is freezed\nTraining results: \nAcc: 0.557, f1: 0.492\nValidation results: \nAcc: 0.571, f1: 0.649\nEpoch: #3\nBert is freezed\nTraining results: \nAcc: 0.572, f1: 0.526\nValidation results: \nAcc: 0.590, f1: 0.683\nEpoch: #4\nBert is freezed\nTraining results: \nAcc: 0.589, f1: 0.565\nValidation results: \nAcc: 0.590, f1: 0.704\nEpoch: #5\nBert is freezed\nTraining results: \nAcc: 0.603, f1: 0.602\nValidation results: \nAcc: 0.607, f1: 0.737\nEpoch: #6\nBert is not freezed\nTraining results: \nAcc: 0.860, f1: 0.859\nValidation results: \nAcc: 0.828, f1: 0.898\nEpoch: #7\nBert is not freezed\nTraining results: \nAcc: 0.919, f1: 0.919\nValidation results: \nAcc: 0.855, f1: 0.916\nEpoch: #8\nBert is not freezed\nTraining results: \nAcc: 0.938, f1: 0.937\nValidation results: \nAcc: 0.853, f1: 0.915\nEpoch: #9\nBert is not freezed\nTraining results: \nAcc: 0.949, f1: 0.949\nValidation results: \nAcc: 0.841, f1: 0.905\nEpoch: #10\nBert is not freezed\nTraining results: \nAcc: 0.969, f1: 0.969\nValidation results: \nAcc: 0.860, f1: 0.920\nEpoch: #11\nBert is not freezed\nmix_up_on\nTraining results: \nAcc: 0.974, f1: 0.974\nValidation results: \nAcc: 0.853, f1: 0.916\nEpoch: #12\nBert is not freezed\nmix_up_on\nTraining results: \nAcc: 0.974, f1: 0.975\nValidation results: \nAcc: 0.858, f1: 0.918\nEpoch: #13\nBert is not freezed\nmix_up_on\nTraining results: \nAcc: 0.974, f1: 0.975\nValidation results: \nAcc: 0.850, f1: 0.911\nEpoch: #14\nBert is not freezed\nmix_up_on\nTraining results: \nAcc: 0.983, f1: 0.983\nValidation results: \nAcc: 0.857, f1: 0.917\nEpoch: #15\nBert is not freezed\nmix_up_on\nTraining results: \nAcc: 0.986, f1: 0.986\nValidation results: \nAcc: 0.852, f1: 0.915\nEpoch: #16\nBert is not freezed\nmix_up_on\nTraining results: \nAcc: 0.978, f1: 0.977\nValidation results: \nAcc: 0.834, f1: 0.903\nEpoch: #17\nBert is not freezed\nmix_up_on\nTraining results: \nAcc: 0.990, f1: 0.990\nValidation results: \nAcc: 0.859, f1: 0.918\nEpoch: #18\nBert is not freezed\nmix_up_on\nTraining results: \nAcc: 0.988, f1: 0.988\nValidation results: \nAcc: 0.852, f1: 0.916\nEpoch: #19\nBert is not freezed\nmix_up_on\nTraining results: \nAcc: 0.991, f1: 0.991\nValidation results: \nAcc: 0.852, f1: 0.913\nEpoch: #20\nBert is not freezed\nmix_up_on\nTraining results: \nAcc: 0.989, f1: 0.989\nValidation results: \nAcc: 0.845, f1: 0.910\n","output_type":"stream"},{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"0.8600746268656716"},"metadata":{}}]},{"cell_type":"code","source":"lam = 0.95\nif_freeze_bert = False\n\nfor i in tqdm(range(5)):\n    print(\"Epoch: #{}\".format(i+1))\n    \n    training_step_with_mixup(train_dataloader, model_mixup, optimizer, loss_fn, if_freeze_bert, lam)\n    train_acc, train_f1 = validation_step(train_dataloader, model_mixup, loss_fn)\n    val_acc, val_f1 = validation_step(eval_dataloader, model_mixup, loss_fn)\n    \n    print(\"Training results: \")\n    print(\"Acc: {:.3f}, f1: {:.3f}\".format(train_acc, train_f1))\n    \n    print(\"Validation results: \")\n    print(\"Acc: {:.3f}, f1: {:.3f}\".format(val_acc, val_f1))\n    \n    if val_acc > best_acc:\n        best_acc = val_acc    \n        torch.save(model_mixup, path)\n        \nbest_acc","metadata":{"execution":{"iopub.status.busy":"2024-06-12T11:30:34.081805Z","iopub.execute_input":"2024-06-12T11:30:34.082135Z","iopub.status.idle":"2024-06-12T11:41:48.580447Z","shell.execute_reply.started":"2024-06-12T11:30:34.082110Z","shell.execute_reply":"2024-06-12T11:41:48.579497Z"},"trusted":true},"execution_count":53,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39c9390c2aec4d01b98994ebc04b311d"}},"metadata":{}},{"name":"stdout","text":"Epoch: #1\nTraining results: \nAcc: 0.994, f1: 0.994\nValidation results: \nAcc: 0.854, f1: 0.916\nEpoch: #2\nTraining results: \nAcc: 0.994, f1: 0.994\nValidation results: \nAcc: 0.857, f1: 0.918\nEpoch: #3\nTraining results: \nAcc: 0.993, f1: 0.993\nValidation results: \nAcc: 0.856, f1: 0.917\nEpoch: #4\nTraining results: \nAcc: 0.995, f1: 0.995\nValidation results: \nAcc: 0.851, f1: 0.915\nEpoch: #5\nTraining results: \nAcc: 0.996, f1: 0.996\nValidation results: \nAcc: 0.850, f1: 0.913\n","output_type":"stream"},{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"0.8600746268656716"},"metadata":{}}]},{"cell_type":"code","source":"model_mixup = BertWithClassifierMixUp(linear_size=hidden_layers)\nmodel_mixup.to(device)\noptimizer = AdamW(model_mixup.parameters(), lr=learning_rate)\nloss_fn = nn.BCELoss()\n\ntqdm.pandas()\n\nbest_acc, best_f1 = 0, 0\npath = './best_model.pt'\nif_freeze_bert = False\nlam = 1\n\nfor i in tqdm(range(num_of_epochs)):\n    print(\"Epoch: #{}\".format(i+1))\n\n    if i < 5:\n        if_freeze_bert = True\n        print(\"Bert is freezed\")\n        lam = 1\n    else:\n        if_freeze_bert = False\n        lam = 0.95\n        print(\"Bert is not freezed\")\n    \n    training_step_with_mixup(train_dataloader, model_mixup, optimizer, loss_fn, if_freeze_bert, lam)\n    train_acc, train_f1 = validation_step(train_dataloader, model_mixup, loss_fn)\n    val_acc, val_f1 = validation_step(eval_dataloader, model_mixup, loss_fn)\n    \n    print(\"Training results: \")\n    print(\"Acc: {:.3f}, f1: {:.3f}\".format(train_acc, train_f1))\n    \n    print(\"Validation results: \")\n    print(\"Acc: {:.3f}, f1: {:.3f}\".format(val_acc, val_f1))\n    \n    if val_acc > best_acc:\n        best_acc = val_acc    \n        torch.save(model_mixup, path)\n        \nbest_acc","metadata":{"execution":{"iopub.status.busy":"2024-06-12T12:30:34.415043Z","iopub.execute_input":"2024-06-12T12:30:34.415391Z","iopub.status.idle":"2024-06-12T12:53:09.800488Z","shell.execute_reply.started":"2024-06-12T12:30:34.415363Z","shell.execute_reply":"2024-06-12T12:53:09.799453Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2044017177d4df18ce8386780db219e"}},"metadata":{}},{"name":"stdout","text":"Epoch: #1\nBert is freezed\nTraining results: \nAcc: 0.508, f1: 0.368\nValidation results: \nAcc: 0.504, f1: 0.507\nEpoch: #2\nBert is freezed\nTraining results: \nAcc: 0.538, f1: 0.441\nValidation results: \nAcc: 0.536, f1: 0.588\nEpoch: #3\nBert is freezed\nTraining results: \nAcc: 0.578, f1: 0.541\nValidation results: \nAcc: 0.581, f1: 0.681\nEpoch: #4\nBert is freezed\nTraining results: \nAcc: 0.554, f1: 0.476\nValidation results: \nAcc: 0.561, f1: 0.630\nEpoch: #5\nBert is freezed\nTraining results: \nAcc: 0.571, f1: 0.518\nValidation results: \nAcc: 0.574, f1: 0.656\nEpoch: #6\nBert is not freezed\nTraining results: \nAcc: 0.879, f1: 0.879\nValidation results: \nAcc: 0.850, f1: 0.912\nEpoch: #7\nBert is not freezed\nTraining results: \nAcc: 0.914, f1: 0.914\nValidation results: \nAcc: 0.860, f1: 0.920\nEpoch: #8\nBert is not freezed\nTraining results: \nAcc: 0.943, f1: 0.943\nValidation results: \nAcc: 0.855, f1: 0.916\nEpoch: #9\nBert is not freezed\nTraining results: \nAcc: 0.959, f1: 0.959\nValidation results: \nAcc: 0.852, f1: 0.914\nEpoch: #10\nBert is not freezed\nTraining results: \nAcc: 0.969, f1: 0.969\nValidation results: \nAcc: 0.859, f1: 0.919\nEpoch: #11\nBert is not freezed\nTraining results: \nAcc: 0.981, f1: 0.981\nValidation results: \nAcc: 0.859, f1: 0.919\nEpoch: #12\nBert is not freezed\nTraining results: \nAcc: 0.985, f1: 0.985\nValidation results: \nAcc: 0.854, f1: 0.915\nEpoch: #13\nBert is not freezed\nTraining results: \nAcc: 0.986, f1: 0.986\nValidation results: \nAcc: 0.845, f1: 0.910\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"0.8600746268656716"},"metadata":{}}]},{"cell_type":"code","source":"model_mixup = BertWithClassifierMixUp(linear_size=hidden_layers)\nmodel_mixup.to(device)\noptimizer = AdamW(model_mixup.parameters(), lr=learning_rate)\nloss_fn = nn.BCELoss()\n\ntqdm.pandas()\n\nbest_acc, best_f1 = 0, 0\npath = './best_model.pt'\nif_freeze_bert = False\nlam = 1\n\nfor i in tqdm(range(num_of_epochs)):\n    print(\"Epoch: #{}\".format(i+1))\n\n    if i < 5:\n        if_freeze_bert = True\n        print(\"Bert is freezed\")\n        lam = 1\n    else:\n        if_freeze_bert = False\n        lam = 1\n        print(\"Bert is not freezed\")\n    \n    training_step_with_mixup(train_dataloader, model_mixup, optimizer, loss_fn, if_freeze_bert, lam)\n    train_acc, train_f1 = validation_step(train_dataloader, model_mixup, loss_fn)\n    val_acc, val_f1 = validation_step(eval_dataloader, model_mixup, loss_fn)\n    \n    print(\"Training results: \")\n    print(\"Acc: {:.3f}, f1: {:.3f}\".format(train_acc, train_f1))\n    \n    print(\"Validation results: \")\n    print(\"Acc: {:.3f}, f1: {:.3f}\".format(val_acc, val_f1))\n    \n    if val_acc > best_acc:\n        best_acc = val_acc    \n        torch.save(model, path)\n        \nbest_acc","metadata":{"execution":{"iopub.status.busy":"2024-06-12T12:56:09.676727Z","iopub.execute_input":"2024-06-12T12:56:09.677423Z","iopub.status.idle":"2024-06-12T13:18:47.923309Z","shell.execute_reply.started":"2024-06-12T12:56:09.677391Z","shell.execute_reply":"2024-06-12T13:18:47.922404Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96fa2dafed0243d3803b74d456c162e2"}},"metadata":{}},{"name":"stdout","text":"Epoch: #1\nBert is freezed\nTraining results: \nAcc: 0.502, f1: 0.357\nValidation results: \nAcc: 0.497, f1: 0.496\nEpoch: #2\nBert is freezed\nTraining results: \nAcc: 0.503, f1: 0.360\nValidation results: \nAcc: 0.499, f1: 0.499\nEpoch: #3\nBert is freezed\nTraining results: \nAcc: 0.514, f1: 0.387\nValidation results: \nAcc: 0.503, f1: 0.512\nEpoch: #4\nBert is freezed\nTraining results: \nAcc: 0.541, f1: 0.459\nValidation results: \nAcc: 0.534, f1: 0.584\nEpoch: #5\nBert is freezed\nTraining results: \nAcc: 0.538, f1: 0.448\nValidation results: \nAcc: 0.534, f1: 0.578\nEpoch: #6\nBert is not freezed\nTraining results: \nAcc: 0.874, f1: 0.873\nValidation results: \nAcc: 0.838, f1: 0.906\nEpoch: #7\nBert is not freezed\nTraining results: \nAcc: 0.917, f1: 0.917\nValidation results: \nAcc: 0.855, f1: 0.917\nEpoch: #8\nBert is not freezed\nTraining results: \nAcc: 0.927, f1: 0.926\nValidation results: \nAcc: 0.838, f1: 0.904\nEpoch: #9\nBert is not freezed\nTraining results: \nAcc: 0.946, f1: 0.946\nValidation results: \nAcc: 0.851, f1: 0.913\nEpoch: #10\nBert is not freezed\nTraining results: \nAcc: 0.967, f1: 0.968\nValidation results: \nAcc: 0.852, f1: 0.915\nEpoch: #11\nBert is not freezed\nTraining results: \nAcc: 0.964, f1: 0.964\nValidation results: \nAcc: 0.825, f1: 0.895\nEpoch: #12\nBert is not freezed\nTraining results: \nAcc: 0.973, f1: 0.973\nValidation results: \nAcc: 0.821, f1: 0.894\nEpoch: #13\nBert is not freezed\nTraining results: \nAcc: 0.977, f1: 0.977\nValidation results: \nAcc: 0.829, f1: 0.900\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"0.855410447761194"},"metadata":{}}]},{"cell_type":"code","source":"from torch import nn\n\nin_features = 768\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n\nclass BertWithClassifierMixUp(nn.Module):\n    def __init__(self, linear_size):\n        super(BertWithClassifierMixUp, self).__init__()\n        self.bert = BertModel.from_pretrained(\"bert-base-cased\")\n        self.head = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(in_features=in_features, out_features=linear_size),\n            nn.BatchNorm1d(num_features=linear_size),\n            nn.Dropout(p=0.8),\n            nn.Linear(in_features=linear_size, out_features=1),\n            # nn.BatchNorm1d(num_features=1),\n            nn.Sigmoid()\n        )\n        \n#     def forward(self, tokens, attention_mask):\n#         bert_output = self.bert(input_ids=tokens, attention_mask=attention_mask)\n#         y = self.head(bert_output[1]) \n#         return y\n    \n    def forward(self, tokens1, attention_mask1, tokens2, attention_mask2, lam):\n        bert_output1 = self.bert(input_ids=tokens1, attention_mask=attention_mask1)\n        bert_output2 = self.bert(input_ids=tokens2, attention_mask=attention_mask2)\n    \n        bert_output = lam * bert_output1[1] + (1.0 - lam) * bert_output2[1]\n    \n        y = self.head(bert_output) \n        return y\n        \n    def freeze_bert(self):\n        for param in self.bert.named_parameters():\n            param[1].requires_grad=False\n    \n    def unfreeze_bert(self):\n        for param in self.bert.named_parameters():\n            param[1].requires_grad=True\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-13T19:38:20.044921Z","iopub.execute_input":"2024-06-13T19:38:20.045316Z","iopub.status.idle":"2024-06-13T19:38:20.698856Z","shell.execute_reply.started":"2024-06-13T19:38:20.045272Z","shell.execute_reply":"2024-06-13T19:38:20.698046Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def training_step_with_mixup(dataloader, model, optimizer, loss_fn, if_freeze_bert, lam):\n    \"\"\"Method to train the model\"\"\"\n    \n    model.train()\n    model.freeze_bert() if if_freeze_bert else model.unfreeze_bert()\n      \n    epoch_loss = 0\n    size = len(dataloader.dataset)\n \n    for i, batch in enumerate(dataloader):\n        \n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        indexes = torch.randperm(len(input_ids)).to(device)\n#         print(input_ids.shape)\n    \n        model_answer = model(input_ids, attention_mask, input_ids[indexes], attention_mask[indexes], lam)\n    \n        outputs = torch.flatten(model_answer)\n        \n        mixup_labels = lam * labels.float() + (1.0 - lam) * labels.float()[indexes]\n        \n        optimizer.zero_grad()\n        loss = loss_fn(outputs, mixup_labels)\n        epoch_loss += loss.item()\n        loss.backward()\n        optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T20:37:45.872272Z","iopub.execute_input":"2024-06-13T20:37:45.872663Z","iopub.status.idle":"2024-06-13T20:37:45.881360Z","shell.execute_reply.started":"2024-06-13T20:37:45.872633Z","shell.execute_reply":"2024-06-13T20:37:45.880262Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"def validation_step_mixup(dataloader, model, loss_fn):\n    \"\"\"Method to test the model's accuracy and loss on the validation set\"\"\"\n    \n    model.eval()\n    model.freeze_bert()\n    \n    size = len(dataloader)\n    f1, acc = 0, 0\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            X = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            y = batch['labels'].to(device)\n                  \n            pred = model(X, attention_mask, X, attention_mask, 1)\n            \n            acc_batch, f1_batch = eval_prediction(y.float(), pred)                        \n            acc += acc_batch\n            f1 += f1_batch\n\n        acc = acc/size\n        f1 = f1/size\n                \n    return acc, f1","metadata":{"execution":{"iopub.status.busy":"2024-06-13T20:05:10.756268Z","iopub.execute_input":"2024-06-13T20:05:10.757003Z","iopub.status.idle":"2024-06-13T20:05:10.764368Z","shell.execute_reply.started":"2024-06-13T20:05:10.756970Z","shell.execute_reply":"2024-06-13T20:05:10.763329Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"model_mixup = BertWithClassifierMixUp(linear_size=hidden_layers)\nmodel_mixup.to(device)\noptimizer = AdamW(model_mixup.parameters(), lr=learning_rate)\nloss_fn = nn.BCELoss()\n\ntqdm.pandas()\n\nbest_acc, best_f1 = 0, 0\npath = './best_model.pt'\nif_freeze_bert = False\nlam = 1\n\nfor i in tqdm(range(num_of_epochs)):\n    print(\"Epoch: #{}\".format(i+1))\n\n    if i < 3:\n        if_freeze_bert = True\n        print(\"Bert is freezed\")\n        lam = 1\n    else:\n        if_freeze_bert = False\n        print(\"Bert is not freezed\")\n        \n    if i >= 6:\n        print(\"Mix Up\")\n        lam = 0.9\n    \n    training_step_with_mixup(train_dataloader, model_mixup, optimizer, loss_fn, if_freeze_bert, lam)\n    train_acc, train_f1 = validation_step_mixup(train_dataloader, model_mixup, loss_fn)\n    val_acc, val_f1 = validation_step_mixup(eval_dataloader, model_mixup, loss_fn)\n    \n    print(\"Training results: \")\n    print(\"Acc: {:.3f}, f1: {:.3f}\".format(train_acc, train_f1))\n    \n    print(\"Validation results: \")\n    print(\"Acc: {:.3f}, f1: {:.3f}\".format(val_acc, val_f1))\n    \n    if val_acc > best_acc:\n        best_acc = val_acc    \n#         torch.save(model_mixup, path)\n        \nbest_acc","metadata":{"execution":{"iopub.status.busy":"2024-06-13T20:05:11.775195Z","iopub.execute_input":"2024-06-13T20:05:11.776056Z","iopub.status.idle":"2024-06-13T20:34:18.226131Z","shell.execute_reply.started":"2024-06-13T20:05:11.776023Z","shell.execute_reply":"2024-06-13T20:34:18.225191Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a83b750191d444c1b3c4777d2f961d88"}},"metadata":{}},{"name":"stdout","text":"Epoch: #1\nBert is freezed\nTraining results: \nAcc: 0.500, f1: 0.351\nValidation results: \nAcc: 0.497, f1: 0.496\nEpoch: #2\nBert is freezed\nTraining results: \nAcc: 0.515, f1: 0.387\nValidation results: \nAcc: 0.504, f1: 0.518\nEpoch: #3\nBert is freezed\nTraining results: \nAcc: 0.541, f1: 0.451\nValidation results: \nAcc: 0.528, f1: 0.578\nEpoch: #4\nBert is not freezed\nTraining results: \nAcc: 0.855, f1: 0.854\nValidation results: \nAcc: 0.832, f1: 0.902\nEpoch: #5\nBert is not freezed\nTraining results: \nAcc: 0.918, f1: 0.918\nValidation results: \nAcc: 0.848, f1: 0.913\nEpoch: #6\nBert is not freezed\nTraining results: \nAcc: 0.949, f1: 0.949\nValidation results: \nAcc: 0.865, f1: 0.922\nEpoch: #7\nBert is not freezed\nMix Up\nTraining results: \nAcc: 0.959, f1: 0.959\nValidation results: \nAcc: 0.860, f1: 0.920\nEpoch: #8\nBert is not freezed\nMix Up\nTraining results: \nAcc: 0.965, f1: 0.965\nValidation results: \nAcc: 0.839, f1: 0.906\nEpoch: #9\nBert is not freezed\nMix Up\nTraining results: \nAcc: 0.976, f1: 0.976\nValidation results: \nAcc: 0.854, f1: 0.915\nEpoch: #10\nBert is not freezed\nMix Up\nTraining results: \nAcc: 0.980, f1: 0.980\nValidation results: \nAcc: 0.850, f1: 0.913\nEpoch: #11\nBert is not freezed\nMix Up\nTraining results: \nAcc: 0.985, f1: 0.985\nValidation results: \nAcc: 0.851, f1: 0.914\nEpoch: #12\nBert is not freezed\nMix Up\nTraining results: \nAcc: 0.988, f1: 0.988\nValidation results: \nAcc: 0.854, f1: 0.917\nEpoch: #13\nBert is not freezed\nMix Up\nTraining results: \nAcc: 0.988, f1: 0.988\nValidation results: \nAcc: 0.851, f1: 0.913\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"0.8647388059701493"},"metadata":{}}]},{"cell_type":"code","source":"model_mixup = BertWithClassifierMixUp(linear_size=hidden_layers)\nmodel_mixup.to(device)\noptimizer = AdamW(model_mixup.parameters(), lr=learning_rate)\nloss_fn = nn.BCELoss()\n\ntqdm.pandas()\n\nbest_acc, best_f1 = 0, 0\npath = './best_model.pt'\nif_freeze_bert = False\nlam = 1\n\nnum_of_epochs= 13\n\nfor i in tqdm(range(num_of_epochs)):\n    print(\"Epoch: #{}\".format(i+1))\n\n    if i < 3:\n        if_freeze_bert = True\n        print(\"Bert is freezed\")\n        lam = 1\n    else:\n        if_freeze_bert = False\n        print(\"Bert is not freezed\")\n        \n    if i >= 6:\n        print(\"Mix Up\")\n        lam = 0.5\n    \n    training_step_with_mixup(train_dataloader, model_mixup, optimizer, loss_fn, if_freeze_bert, lam)\n    train_acc, train_f1 = validation_step_mixup(train_dataloader, model_mixup, loss_fn)\n    val_acc, val_f1 = validation_step_mixup(eval_dataloader, model_mixup, loss_fn)\n    \n    print(\"Training results: \")\n    print(\"Acc: {:.3f}, f1: {:.3f}\".format(train_acc, train_f1))\n    \n    print(\"Validation results: \")\n    print(\"Acc: {:.3f}, f1: {:.3f}\".format(val_acc, val_f1))\n    \n    if val_acc > best_acc:\n        best_acc = val_acc    \n        torch.save(model_mixup, path)\n        \nbest_acc","metadata":{"execution":{"iopub.status.busy":"2024-06-13T20:38:21.509065Z","iopub.execute_input":"2024-06-13T20:38:21.509745Z","iopub.status.idle":"2024-06-13T21:07:28.432041Z","shell.execute_reply.started":"2024-06-13T20:38:21.509707Z","shell.execute_reply":"2024-06-13T21:07:28.430967Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/13 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8820921172b444408ad78e71a0f3508b"}},"metadata":{}},{"name":"stdout","text":"Epoch: #1\nBert is freezed\nTraining results: \nAcc: 0.569, f1: 0.547\nValidation results: \nAcc: 0.562, f1: 0.668\nEpoch: #2\nBert is freezed\nTraining results: \nAcc: 0.593, f1: 0.585\nValidation results: \nAcc: 0.579, f1: 0.699\nEpoch: #3\nBert is freezed\nTraining results: \nAcc: 0.612, f1: 0.612\nValidation results: \nAcc: 0.604, f1: 0.736\nEpoch: #4\nBert is not freezed\nTraining results: \nAcc: 0.876, f1: 0.877\nValidation results: \nAcc: 0.834, f1: 0.903\nEpoch: #5\nBert is not freezed\nTraining results: \nAcc: 0.918, f1: 0.917\nValidation results: \nAcc: 0.847, f1: 0.912\nEpoch: #6\nBert is not freezed\nTraining results: \nAcc: 0.946, f1: 0.946\nValidation results: \nAcc: 0.855, f1: 0.916\nEpoch: #7\nBert is not freezed\nMix Up\nTraining results: \nAcc: 0.951, f1: 0.951\nValidation results: \nAcc: 0.843, f1: 0.909\nEpoch: #8\nBert is not freezed\nMix Up\nTraining results: \nAcc: 0.963, f1: 0.963\nValidation results: \nAcc: 0.861, f1: 0.921\nEpoch: #9\nBert is not freezed\nMix Up\nTraining results: \nAcc: 0.960, f1: 0.960\nValidation results: \nAcc: 0.844, f1: 0.909\nEpoch: #10\nBert is not freezed\nMix Up\nTraining results: \nAcc: 0.968, f1: 0.968\nValidation results: \nAcc: 0.854, f1: 0.915\nEpoch: #11\nBert is not freezed\nMix Up\nTraining results: \nAcc: 0.975, f1: 0.975\nValidation results: \nAcc: 0.866, f1: 0.923\nEpoch: #12\nBert is not freezed\nMix Up\nTraining results: \nAcc: 0.979, f1: 0.979\nValidation results: \nAcc: 0.854, f1: 0.916\nEpoch: #13\nBert is not freezed\nMix Up\nTraining results: \nAcc: 0.977, f1: 0.977\nValidation results: \nAcc: 0.849, f1: 0.913\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"0.8656716417910447"},"metadata":{}}]},{"cell_type":"code","source":"model_mixup = BertWithClassifierMixUp(linear_size=hidden_layers)\nmodel_mixup.to(device)\noptimizer = AdamW(model_mixup.parameters(), lr=learning_rate)\nloss_fn = nn.BCELoss()\n\ntqdm.pandas()\n\nbest_acc, best_f1 = 0, 0\npath = './best_model.pt'train_dataloader\nif_freeze_bert = False\nlam = 1\n\nnum_of_epochs = 6\n\nfor i in tqdm(range(num_of_epochs)):\n    print(\"Epoch: #{}\".format(i+1))\n\n    if i < 3:\n        if_freeze_bert = True\n        print(\"Bert is freezed\")\n        lam = 1\n    else:\n        if_freeze_bert = False\n        lam = 0.9\n        print(\"Bert is not freezed\")\n        \n    \n    training_step_with_mixup(train_dataloader, model_mixup, optimizer, loss_fn, if_freeze_bert, lam)\n    train_acc, train_f1 = validation_step_mixup(train_dataloader, model_mixup, loss_fn)\n    val_acc, val_f1 = validation_step_mixup(eval_dataloader, model_mixup, loss_fn)\n    \n    print(\"Training results: \")\n    print(\"Acc: {:.3f}, f1: {:.3f}\".format(train_acc, train_f1))\n    \n    print(\"Validation results: \")\n    print(\"Acc: {:.3f}, f1: {:.3f}\".format(val_acc, val_f1))\n    \n    if val_acc > best_acc:\n        best_acc = val_acc    \n#         torch.save(model_mixup, path)\n        \nbest_acc","metadata":{"execution":{"iopub.status.busy":"2024-06-13T21:07:28.433755Z","iopub.execute_input":"2024-06-13T21:07:28.434044Z","iopub.status.idle":"2024-06-13T21:18:53.724747Z","shell.execute_reply.started":"2024-06-13T21:07:28.434020Z","shell.execute_reply":"2024-06-13T21:18:53.723806Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2baef1f1623488bbb99c9cc80086a8b"}},"metadata":{}},{"name":"stdout","text":"Epoch: #1\nBert is freezed\nTraining results: \nAcc: 0.550, f1: 0.484\nValidation results: \nAcc: 0.533, f1: 0.601\nEpoch: #2\nBert is freezed\nTraining results: \nAcc: 0.610, f1: 0.611\nValidation results: \nAcc: 0.614, f1: 0.743\nEpoch: #3\nBert is freezed\nTraining results: \nAcc: 0.603, f1: 0.586\nValidation results: \nAcc: 0.617, f1: 0.732\nEpoch: #4\nBert is not freezed\nTraining results: \nAcc: 0.879, f1: 0.878\nValidation results: \nAcc: 0.848, f1: 0.913\nEpoch: #5\nBert is not freezed\nTraining results: \nAcc: 0.919, f1: 0.919\nValidation results: \nAcc: 0.853, f1: 0.914\nEpoch: #6\nBert is not freezed\nTraining results: \nAcc: 0.922, f1: 0.922\nValidation results: \nAcc: 0.825, f1: 0.893\n","output_type":"stream"},{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"0.8526119402985075"},"metadata":{}}]},{"cell_type":"markdown","source":"Тут последняя модель нужно переобучить под лучшую","metadata":{}},{"cell_type":"code","source":"test_dataloader = DataLoader(\n    tokenized_datasets['test'], shuffle=True, batch_size=8, collate_fn=data_collator\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T21:22:44.171314Z","iopub.execute_input":"2024-06-13T21:22:44.172164Z","iopub.status.idle":"2024-06-13T21:22:44.176397Z","shell.execute_reply.started":"2024-06-13T21:22:44.172134Z","shell.execute_reply":"2024-06-13T21:22:44.175517Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"validation_step_mixup(test_dataloader, model_mixup, loss_fn)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T21:23:22.919365Z","iopub.execute_input":"2024-06-13T21:23:22.919964Z","iopub.status.idle":"2024-06-13T21:23:27.439659Z","shell.execute_reply.started":"2024-06-13T21:23:22.919934Z","shell.execute_reply":"2024-06-13T21:23:27.438785Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"(0.808768656716418, 0.80521499644634)"},"metadata":{}}]}]}